# mem0-supadatabase

Link repo: https://github.com/coleam00/ottomator-agents/tree/main/mem0-agent

============
`<think>`
Let me create a comprehensive README and CHANGELOG for this project. I'll structure it professionally to describe the project, its features, installation instructions, and usage.

For the README, I'll include:

1. Project overview
2. Features
3. Installation instructions
4. Usage
5. Configuration
6. Docker deployment
7. License and credits

For the CHANGELOG, I'll create an initial entry for the 1.0 release with all the features that have been implemented.
`</think>`

# Mem0 Chat Assistant with Database Viewer

## Release 1.0

A fullstack application featuring a memory-powered AI chat assistant built with Streamlit, Supabase vector storage, and OpenAI. This project demonstrates how AI can maintain context and remember past conversations across sessions using vector embeddings.

![Mem0 Chat Assistant](screenshot.png)

## Features

### üß† Memory-Powered Chat

- Long-term memory storage using vector embeddings
- Contextual responses based on conversation history
- Memory persistence across sessions

### üîç Database Explorer

- View and explore the vector database
- Browse all tables in all schemas
- Examine memory contents and metadata

### üîí User Authentication

- Secure login and registration using Supabase
- User-specific memories and conversations
- Profile management

### üîß Technical Features

- Vector embedding storage for semantic search
- Containerized deployment with Docker
- Error handling and graceful degradation

## Quick Start

### Using Docker Compose

```bash
# Clone the repository
git clone https://github.com/DoanNgocCuong/mem0-supadatabase.git
cd mem0-supadatabase

# Configure your environment variables
cp .env.example .env
# Edit .env with your Supabase and OpenAI credentials

# Start the applications
docker-compose up -d
```

### Accessing the Applications

- Chat Application: http://localhost:25048
- Database Viewer: http://localhost:25049

## Configuration

Edit the `.env` file to configure the following:

```
OPENAI_API_KEY=your_openai_api_key
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key
DATABASE_URL=your_database_connection_string
MODEL_CHOICE=gpt-4o-mini  # or another OpenAI model
```

## Architecture

This project consists of two main components:

1. **Mem0 Chat Assistant**: A Streamlit application that provides a chat interface with memory-powered responses.
2. **Database Viewer**: A separate Streamlit application for exploring the vector database.

Both components connect to the same Supabase backend for data storage and authentication.

## Development

### Requirements

- Python 3.10+
- Supabase account
- OpenAI API key

### Local Setup

```bash
# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run the applications
streamlit run v3-streamlit-supabase-mem0.py
streamlit run v3_view_database.py
```

## Troubleshooting

### Common Issues

- **Database Timeout**: Increase PostgreSQL statement_timeout value
- **Authentication Issues**: Ensure Supabase credentials are correct
- **Memory Creation Error**: Check database connection and permissions

## Credits

Developed by [Doan Ngoc Cuong](https://github.com/DoanNgocCuong)

Based on the [Ottomator Mem0 Agent](https://github.com/coleam00/ottomator-agents/tree/main/mem0-agent)

## License

MIT License

```markdown:CHANGELOG.md
# Changelog

All notable changes to this project will be documented in this file.

## [1.0.0] - 2024-03-30

### Added
- Initial release of Mem0 Chat Assistant
- Memory-powered conversations using vector embeddings
- User authentication with Supabase
- Database viewer for exploring vector storage
- Docker containerization for both applications
- Profile information display for logged in users
- Memory management features (search, add, clear)
- Error handling and graceful degradation
- Configurable OpenAI model selection
- Real-time chat interface with message history
- Author information and GitHub profile link

### Fixed
- PostgreSQL timeout issues with increased timeouts
- Memory collection creation with proper error handling
- Database connection stability across container restarts
- Authentication session persistence
- Metadata extraction and display in Database Viewer
```

Is there anything specific you'd like me to add to the README or CHANGELOG?


---

# Check ng√†y 20/12/2025 - Khi AI ƒë√£ qu√° m·∫°nh ƒë·ªÉ ƒë·ªçc nhanh 1 repo 



ƒêang ph√¢n t√≠ch c√°ch tri·ªÉn khai mem0 trong code hi·ªán t·∫°i. ƒê·ªçc c√°c file ch√≠nh ƒë·ªÉ hi·ªÉu c·∫•u tr√∫c.

[5 tools called]

ƒêang xem th√™m c√°c file trong `studio-integration-version` ƒë·ªÉ hi·ªÉu ƒë·∫ßy ƒë·ªß:

[2 tools called]

T√≥m t·∫Øt c√°ch tri·ªÉn khai mem0 trong code hi·ªán t·∫°i:

## T·ªïng quan tri·ªÉn khai mem0

C√≥ 4 phi√™n b·∫£n tri·ªÉn khai, t·ª´ c∆° b·∫£n ƒë·∫øn n√¢ng cao:

---

### 1. Version 1: Basic Implementation (`v1-basic-mem0.py`)

Tri·ªÉn khai c∆° b·∫£n v·ªõi in-memory storage:

```8:18:v1-basic-mem0.py
config = {
    "llm": {
        "provider": "openai",
        "config": {
            "model": "gpt-4o-mini"
        }
    }
}

openai_client = OpenAI()
memory = Memory.from_config(config)
```

ƒê·∫∑c ƒëi·ªÉm:

- Ch·ªâ c·∫•u h√¨nh LLM (OpenAI)
- Kh√¥ng c√≥ vector store ‚Üí l∆∞u trong memory t·∫°m th·ªùi
- Ph√π h·ª£p ƒë·ªÉ test nhanh

---

### 2. Version 2: Supabase Integration (`v2-supabase-mem0.py`)

Th√™m Supabase l√†m vector store:

```9:26:v2-supabase-mem0.py
config = {
    "llm": {
        "provider": "openai",
        "config": {
            "model": os.getenv('MODEL_CHOICE', 'gpt-4o-mini')
        }
    },
    "vector_store": {
        "provider": "supabase",
        "config": {
            "connection_string": os.environ['DATABASE_URL'],
            "collection_name": "memories"
        }
    }  
}

openai_client = OpenAI()
memory = Memory.from_config(config)
```

ƒê·∫∑c ƒëi·ªÉm:

- Th√™m `vector_store` v·ªõi provider `supabase`
- L∆∞u memories v√†o PostgreSQL (Supabase)
- D·ªØ li·ªáu t·ªìn t·∫°i l√¢u d√†i

---

### 3. Version 3: Streamlit Web App (`v3-streamlit-supabase-mem0.py`)

Phi√™n b·∫£n ƒë·∫ßy ƒë·ªß v·ªõi UI v√† authentication:

#### a) C·∫•u h√¨nh Memory v·ªõi Supabase:

```65:100:v3-streamlit-supabase-mem0.py
        # T·∫°o config cho Memory - lo·∫°i b·ªè create_collection
        config = {
            "llm": {
                "provider": "openai",
                "config": {
                    "model": MODEL_CHOICE
                }
            },
            "vector_store": {
                "provider": "supabase",
                "config": {
                    "connection_string": conn_str,
                    "collection_name": "memories_new",
                    "embedding_model_dims": 1536  # S·ªë chi·ªÅu c·ªßa OpenAI text-embedding-ada-002
                }
            }  
        }
      
        # Th·ª≠ t·∫°o collection tr∆∞·ªõc khi kh·ªüi t·∫°o Memory
        try:
            import vecs
            db = vecs.create_client(conn_str)
            # Ki·ªÉm tra n·∫øu collection ƒë√£ t·ªìn t·∫°i
            try:
                # S·ª≠ d·ª•ng get_or_create_collection thay v√¨ create_collection
                db.get_or_create_collection(
                    name="memories_new",
                    dimension=1536
                )
                st.success("Collection ƒë√£ ƒë∆∞·ª£c t·∫°o/truy c·∫≠p th√†nh c√¥ng!")
            except Exception as e:
                st.warning(f"Kh√¥ng th·ªÉ t·∫°o collection: {str(e)}")
        except Exception as e:
            st.warning(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi vecs: {str(e)}")
      
        return Memory.from_config(config)
```

ƒêi·ªÉm n·ªïi b·∫≠t:

- T·∫°o collection tr∆∞·ªõc b·∫±ng `vecs` v·ªõi `get_or_create_collection`
- Ch·ªâ ƒë·ªãnh `embedding_model_dims: 1536` (OpenAI ada-002)
- X·ª≠ l√Ω timeout cho database connection
- C√≥ fallback n·∫øu kh·ªüi t·∫°o th·∫•t b·∫°i

#### b) Workflow s·ª≠ d·ª•ng Memory:

```166:203:v3-streamlit-supabase-mem0.py
def chat_with_memories(message, user_id):
    try:
        # Retrieve relevant memories
        with st.spinner("Searching memories..."):
            try:
                relevant_memories = memory.search(query=message, user_id=user_id, limit=3)
                memories_str = "\n".join(f"- {entry['memory']}" for entry in relevant_memories["results"])
            except Exception as e:
                st.error(f"Error retrieving memories: {str(e)}")
                memories_str = "(No memories available)"
      
        # Generate Assistant response
        system_prompt = f"You are a helpful AI assistant with memory. Answer the question based on the query and user's memories.\nUser Memories:\n{memories_str}"
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": message}]
      
        with st.spinner("Thinking..."):
            try:
                response = openai_client.chat.completions.create(
                    model="gpt-4o-mini", 
                    messages=messages,
                    timeout=60  # Increase timeout to 60 seconds
                )
                assistant_response = response.choices[0].message.content
            except Exception as e:
                st.error(f"Error generating response: {str(e)}")
                return "I'm sorry, I couldn't generate a response at this time. Please try again later."

        # Create new memories from the conversation
        try:
            messages.append({"role": "assistant", "content": assistant_response})
            memory.add(messages, user_id=user_id)
        except Exception as e:
            st.warning(f"Could not save this conversation to memory: {str(e)}")

        return assistant_response
    except Exception as e:
        st.error(f"General error: {str(e)}")
        return "Sorry, an error occurred. Please try again later."
```

Quy tr√¨nh:

1. Search: `memory.search(query, user_id, limit=3)` ƒë·ªÉ l·∫•y memories li√™n quan
2. Generate: ƒê∆∞a memories v√†o system prompt, g·ªçi OpenAI
3. Store: `memory.add(messages, user_id)` ƒë·ªÉ l∆∞u cu·ªôc h·ªôi tho·∫°i

#### c) Qu·∫£n l√Ω Memory:

```276:281:v3-streamlit-supabase-mem0.py
            # Memory management options
            st.subheader("Memory Management")
            if st.button("Clear All Memories"):
                memory.clear(user_id=user.id)
                st.success("All memories cleared!")
                st.session_state.messages = []
                st.rerun()
```

---

### 4. Studio Integration Version (`studio-integration-version/`)

T√≠ch h·ª£p v·ªõi Pydantic AI v√† FastAPI:

#### a) Agent v·ªõi Pydantic AI:

```27:36:studio-integration-version/mem0_agent.py
mem0_agent = Agent(
    OpenAIModel(llm),
    system_prompt=f'You are a helpful AI. Answer the question based on query and memories. The current date is: {datetime.now().strftime("%Y-%m-%d")}',
    deps_type=Mem0Deps,
    retries=2
)

@mem0_agent.system_prompt  
def add_memories(ctx: RunContext[str]) -> str:
    return f"\nUser Memories:\n{ctx.deps.memories}"
```

#### b) FastAPI Endpoint v·ªõi Memory:

```168:203:studio-integration-version/mem0_agent_endpoint.py
        # Retrieve relevant memories with Mem0
        try:
            relevant_memories = memory.search(query=request.query, user_id=request.user_id, limit=3)
            memories_str = "\n".join(f"- {entry['memory']}" for entry in relevant_memories["results"])
        except Exception as e:
            print(f"Error retrieving memories: {str(e)}")
            memories_str = "(No memories available)"

        # Initialize agent dependencies
        async with httpx.AsyncClient() as client:
            deps = Mem0Deps(
                memories=memories_str
            )

            # Run the agent with conversation history
            result = await mem0_agent.run(
                request.query,
                message_history=messages,
                deps=deps
            )

        # Store agent's response
        await store_message(
            session_id=request.session_id,
            message_type="ai",
            content=result.data,
            data={"request_id": request.request_id}
        )

        # Update memories based on the last user message and agent response
        try:
            memory_messages = [
                {"role": "user", "content": request.query},
                {"role": "assistant", "content": result.data}
            ]
            memory.add(memory_messages, user_id=request.user_id)      
        except Exception as e:
            print(f"Error adding memory: {str(e)}")
```

---

## T√≥m t·∫Øt ki·∫øn tr√∫c

### C√°c th√†nh ph·∫ßn ch√≠nh:

1. Memory Configuration:

   - LLM Provider: OpenAI (gpt-4o-mini)
   - Vector Store: Supabase (PostgreSQL + pgvector)
   - Collection: `memories_new` ho·∫∑c `memories_api_new`
   - Embedding: 1536 dimensions (OpenAI ada-002)
2. Memory Operations:

   - `memory.search(query, user_id, limit)`: T√¨m ki·∫øm memories
   - `memory.add(messages, user_id)`: L∆∞u cu·ªôc h·ªôi tho·∫°i
   - `memory.clear(user_id)`: X√≥a t·∫•t c·∫£ memories c·ªßa user
3. User Isolation:

   - M·ªói user c√≥ `user_id` ri√™ng
   - Memories ƒë∆∞·ª£c l·ªçc theo `user_id` khi search
4. Error Handling:

   - C√≥ fallback n·∫øu Memory kh·ªüi t·∫°o th·∫•t b·∫°i
   - X·ª≠ l√Ω timeout cho database
   - Try-catch cho c√°c operations

---

## ƒêi·ªÉm m·∫°nh c·ªßa c√°ch tri·ªÉn khai

1. T√°ch bi·ªát theo user (`user_id`)
2. L∆∞u tr·ªØ l√¢u d√†i v·ªõi Supabase
3. T·ª± ƒë·ªông t·∫°o collection n·∫øu ch∆∞a c√≥
4. X·ª≠ l√Ω l·ªói v√† fallback
5. T√≠ch h·ª£p v·ªõi Streamlit v√† FastAPI

B·∫°n mu·ªën t√¥i gi·∫£i th√≠ch s√¢u h∆°n ph·∫ßn n√†o, ho·∫∑c ƒë·ªÅ xu·∫•t c·∫£i thi·ªán?
